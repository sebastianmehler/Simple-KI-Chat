# ğŸ§  Simple-KI-Chat

Ein minimalistischer, lokal laufender Konsolen-Chat mit einer lokalen Sprach-KI Ã¼ber LM Studio.  
Ideal fÃ¼r experimentelle Unterhaltungen, Rollenspiele oder philosophische GesprÃ¤che mit einem empathischen, kreativen Assistenten.

Dieses Projekt ist eine erste AnnÃ¤herung an die Programmiersprache Python und entstand vollstÃ¤ndig im Rahmen sogenannter *Vibe Coding*-Sessions â€“ also durch kreatives Prompt Engineering mit KI-UnterstÃ¼tzung.

## ğŸš€ Features

- Lokale Nutzung eines KI-Modells Ã¼ber LM Studio (OpenAI-kompatibles API)
- GesprÃ¤chsverlauf mit Zeitstempel wird automatisch gespeichert
- UnterstÃ¼tzung fÃ¼r System-Prompts und Modellwahl
- **Antworten werden gestreamt**, d.â€¯h. sie erscheinen in Echtzeit wÃ¤hrend der Generierung
- Farbliche Hervorhebung von <think>-BlÃ¶cken fÃ¼r "Gedanken" der KI
- Einfache Konfiguration Ã¼ber `config.json`

## ğŸ“¦ Voraussetzungen

- Python 3.9+
- Lokaler LM Studio Server mit aktiviertem API-Zugriff
- Ein Modell (z.â€¯B. Mistral, MythoMax etc.), das mit dem OpenAI-kompatiblen Endpunkt verwendet werden kann

## ğŸ”§ Installation

1. Klone das Repository:

```bash
git clone https://github.com/dein-benutzername/simple-ki-chat.git
cd simple-ki-chat
```

2. Installiere die benÃ¶tigten Python-AbhÃ¤ngigkeiten:

```bash
pip install -r requirements.txt
```

> **Hinweis:** Die einzige externe AbhÃ¤ngigkeit ist `requests`. Stelle sicher, dass sie installiert ist:
>
> ```bash
> pip install requests
> ```

3. Erstelle deine Konfigurationsdatei:

```bash
cp config.json.sample config.json
```

> **Wichtig:** Die Datei `config.json.sample` muss in `config.json` umbenannt werden, damit das Programm korrekt startet.

Bearbeite `config.json` nach deinen WÃ¼nschen. Beispiel:

```json
{
  "system_prompt": "Du bist ein hilfreiches, kreatives KI-Modell.",
  "endpoint_url": "http://localhost:1234",
  "max_history": 10
}
```

## ğŸ—¨ï¸ Nutzung

Starte den Konsolenchat mit:

```bash
python chat_console.py
```

Gib deine Nachrichten ein â€“ die KI antwortet live mit farblicher Hervorhebung. Beende den Chat mit `exit` oder `quit`.

Am Ende wird automatisch eine Datei wie `chat_20250419_1530.json` erzeugt, die den kompletten Verlauf enthÃ¤lt.

## ğŸ“ Projektstruktur

```
.
â”œâ”€â”€ chat_console.py        # Hauptprogramm fÃ¼r die Konsoleninteraktion
â”œâ”€â”€ lmstudio_api.py        # API-Kommunikation mit LM Studio (OpenAI-kompatibel)
â”œâ”€â”€ config.json.sample     # Beispielkonfiguration
â”œâ”€â”€ .gitignore             # Ignoriert z.â€¯B. ChatverlÃ¤ufe und `config.json`
â”œâ”€â”€ LICENSE                # Lizenz (falls vorhanden)
â”œâ”€â”€ README.md              # Diese Datei
```

## ğŸ›¡ï¸ Datenschutz

Alle Daten verbleiben lokal. Es findet keine Kommunikation mit externen Servern statt â€“ alles lÃ¤uft Ã¼ber deinen eigenen LM Studio Server.

## ğŸ“œ Lizenz

Siehe [LICENSE](LICENSE).

## ğŸ’¡ Idee oder Beitrag?

Pull Requests, Ideen und Verbesserungen sind willkommen!

---

ğŸ› ï¸ *Diese Dokumentation wurde mithilfe von KI und Prompt Engineering durch den Autor erstellt.*

