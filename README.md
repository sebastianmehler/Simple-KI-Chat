# ğŸ§  Simple-KI-Chat

A minimalist, local console-based chat interface powered by a local language model via LM Studio.  
Ideal for experimental conversations, role-playing, or philosophical exchanges with a creative and empathetic assistant.

This project represents the author's first steps with Python and was created entirely through *Vibe Coding* â€” using prompt engineering and AI-assisted development.

## ğŸš€ Features

- Local use of an AI model via LM Studio (OpenAI-compatible API)
- Automatic saving of chat history with timestamps
- Support for system prompts and model selection
- **Streaming responses** â€” the reply is shown in real time while being generated
- Highlighting of `<think>` blocks to show the model's "thoughts"
- Simple configuration via `config.json`

## ğŸ“¦ Requirements

- Python 3.9+
- A local LM Studio server with API access enabled
- A compatible model (e.g. Mistral, MythoMax) that works with the OpenAI-compatible endpoint

## ğŸ”§ Installation

1. Clone the repository:

```bash
git clone https://github.com/your-username/simple-ki-chat.git
cd simple-ki-chat
```

2. Install the required Python dependencies:

```bash
pip install -r requirements.txt
```

> **Note:** The only external dependency is `requests`. Ensure it is installed:
>
> ```bash
> pip install requests
> ```

3. Create your configuration file:

```bash
cp config.json.sample config.json
```

> **Important:** You must rename `config.json.sample` to `config.json` for the program to run correctly.

Edit `config.json` to your needs. Example:

```json
{
  "system_prompt": "You are a helpful, creative AI model.",
  "endpoint_url": "http://localhost:1234",
  "max_history": 10
}
```

## ğŸ—¨ï¸ Usage

Before starting the chat application, please ensure that LM Studio is properly configured:

1. **Start LM Studio** â€“ Open LM Studio on your machine.
2. **Enable API Access** â€“ In the settings, make sure the OpenAI-compatible API is enabled.
3. **Load a Model** â€“ Either load a model manually (e.g., Mistral, MythoMax) or configure LM Studio to load a default model automatically.
   - If prompted during startup, select the model you'd like to use.
   - Some models may take a moment to load into memory.

Once LM Studio is running and ready:

```bash
python chat_console.py
```

Type your messages â€” the AI will reply in real time with colored highlights. Exit the chat by typing `exit` or `quit`.

At the end of the session, a file such as `chat_20250419_1530.json` will be created, containing the full conversation history.

Start the console chat with:

```bash
python chat_console.py
```

Type your messages â€” the AI will reply in real time with colored highlights. Exit the chat by typing `exit` or `quit`.

At the end of the session, a file such as `chat_20250419_1530.json` will be created, containing the full conversation history.

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ chat_console.py        # Main program for console interaction
â”œâ”€â”€ lmstudio_api.py        # API communication with LM Studio (OpenAI-compatible)
â”œâ”€â”€ config.json.sample     # Sample configuration file
â”œâ”€â”€ .gitignore             # Ignores chat logs and config.json
â”œâ”€â”€ LICENSE                # License information
â”œâ”€â”€ README.md              # This file
```

## ğŸ›¡ï¸ Privacy

All data remains local. There is no communication with external servers â€” everything runs via your local LM Studio server.

## ğŸ“œ License

See [LICENSE](LICENSE).

## ğŸ’¡ Ideas or Contributions?

Pull requests, feedback, and suggestions are welcome!

---

ğŸ› ï¸ *This documentation was written by the author using AI and prompt engineering.*

